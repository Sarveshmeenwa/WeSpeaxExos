{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-29 13:29:00 INFO: Loading these models for language: ar (Arabic):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | padt    |\n",
      "| mwt       | padt    |\n",
      "| pos       | padt    |\n",
      "| lemma     | padt    |\n",
      "| depparse  | padt    |\n",
      "=======================\n",
      "\n",
      "2021-07-29 13:29:00 INFO: Use device: cpu\n",
      "2021-07-29 13:29:00 INFO: Loading: tokenize\n",
      "2021-07-29 13:29:00 INFO: Loading: mwt\n",
      "2021-07-29 13:29:00 INFO: Loading: pos\n",
      "2021-07-29 13:29:02 INFO: Loading: lemma\n",
      "2021-07-29 13:29:02 INFO: Loading: depparse\n",
      "2021-07-29 13:29:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "import libqutrub.conjugator\n",
    "from IPython.core.display import display, HTML\n",
    "import alyahmor.genelex\n",
    "import random\n",
    "from spacy_stanza import StanzaLanguage\n",
    "from spacy import displacy\n",
    "import re\n",
    "import numpy as np\n",
    "nlp = stanza.Pipeline('ar',processors='tokenize,mwt,pos,lemma,depparse', verbose = True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column:\tStatus\n",
    "\n",
    "# ID:\tSentence-level units in PADT often correspond to entire paragraphs and they were obtained automatically. Low-level tokenization (whitespace and punctuation) was done automatically and then hand-corrected. Splitting of fused tokens into syntactic words in Arabic is part of morphological analysis. ElixirFM was used to provide context-independent options, then these results were disambiguated manually.\n",
    "\n",
    "# FORM:\tThe unvocalized surface form is used. Fully vocalized counterpart can be found in the MISC column as Vform attribute.\n",
    "\n",
    "# LEMMA:\tPlausible analyses provided by ElixirFM, manual disambiguation. Lemmas are vocalized. Part of the selection of lemmas was also word sense disambiguation of the lexemes, providing English equivalents (see the Gloss attribute of the MISC column).\n",
    "\n",
    "# UPOSTAG:\tConverted automatically from XPOSTAG (via Interset); human checking of patterns revealed by automatic consistency tests.\n",
    "\n",
    "# XPOSTAG:\tManual selection from possibilities provided by ElixirFM.\n",
    "# FEATS:\tConverted automatically from XPOSTAG (via Interset); human checking of patterns revealed by automatic consistency tests.\n",
    "\n",
    "# HEAD:\tOriginal PADT annotation is manual. Automatic conversion to UD; human checking of patterns revealed by automatic consistency tests.\n",
    "\n",
    "# DEPREL:\tOriginal PDT annotation is manual. Automatic conversion to UD; human checking of patterns revealed by automatic consistency tests.\n",
    "\n",
    "# DEPS:\t— (currently unused)\n",
    "\n",
    "# MISC:\tInformation about token spacing taken from PADT annotation. Additional word attributes provided by morphological analysis (i.e. ElixirFM rules + manual disambiguation): Vform (fully vocalized Arabic form), Translit (Latin transliteration of word form), LTranslit (Latin transliteration of lemma), Root (word root), Gloss (English translation of lemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['التمور', 'النموز']\n"
     ]
    }
   ],
   "source": [
    "def dotCountChanger(word):\n",
    "    \"\"\" for each letter in a word,\n",
    "    change it with a similar character but with different number of dots, then add \n",
    "    each of the character replaced and the new character to the lists list_of_chrs_to_change and list_of_chrs_to_add\n",
    "    respectively\"\"\"\n",
    "    list_of_distractors=[]\n",
    "    chr_to_change=\"\"\n",
    "    chr_to_add=\"\"\n",
    "    for i in word:\n",
    "        chr_to_change=\"\"\n",
    "        chr_to_add=\"\"\n",
    "        if \"ت\" == i:\n",
    "            chr_to_add= \"ث\"\n",
    "            chr_to_change= \"ت\"\n",
    "        if \"ن\" == i:\n",
    "            chr_to_add= \"ت\" \n",
    "            chr_to_change= \"ن\"\n",
    "        if \"س\" == i:\n",
    "            chr_to_add= \"ش\" \n",
    "            chr_to_change=\"س\"\n",
    "        if \"ف\" == i:\n",
    "            chr_to_add= \"ق\"\n",
    "            chr_to_change=\"ف\"\n",
    "        if \"ب\" == i:\n",
    "            chr_to_add= \"ي\"\n",
    "            chr_to_change=\"ب\"\n",
    "        if \"ح\" == i:\n",
    "            chr_to_add= \"خ\" \n",
    "            chr_to_change=\"ح\"\n",
    "        if \"خ\" == i:\n",
    "            chr_to_add= \"ج\" \n",
    "            chr_to_change=\"خ\" \n",
    "        if \"ز\" == i:\n",
    "            chr_to_add= \"ر\" \n",
    "            chr_to_change=\"ز\"\n",
    "        if \"ط\" == i:\n",
    "            chr_to_add= \"ظ\"\n",
    "            chr_to_change=\"ط\" \n",
    "        if \"د\" == i:\n",
    "            chr_to_add= \"ذ\" \n",
    "            chr_to_change=\"د\"\n",
    "        if \"ع\" == i:\n",
    "            chr_to_add= \"غ\"\n",
    "            chr_to_change=\"ع\"\n",
    "        if \"ص\" == i:\n",
    "            chr_to_add= \"ض\"\n",
    "            chr_to_change=\"ص\"\n",
    "\n",
    "        if \"ث\" == i:\n",
    "            chr_to_add= \"ت\" \n",
    "            chr_to_change=\"ث\" \n",
    "        if \"ش\" == i:\n",
    "            chr_to_add= \"س\" \n",
    "            chr_to_change=\"ش\"\n",
    "        if \"ق\" == i:\n",
    "            chr_to_add= \"ف\" \n",
    "            chr_to_change=\"ق\"\n",
    "        if \"ي\" == i:\n",
    "            chr_to_add= \"ب\"\n",
    "            chr_to_change=\"ي\"\n",
    "        if \"خ\" == i:\n",
    "            chr_to_add= \"ح\" \n",
    "            chr_to_change=\"خ\"\n",
    "        if \"ج\" == i:\n",
    "            chr_to_add= \"خ\" \n",
    "            chr_to_change=\"ج\"\n",
    "        if \"ر\" == i:\n",
    "            chr_to_add= \"ز\"  \n",
    "            chr_to_change=\"ر\"\n",
    "        if \"ظ\" == i:\n",
    "            chr_to_add= \"ط\" \n",
    "            chr_to_change=\"ظ\"\n",
    "        if \"ذ\" == i:\n",
    "            chr_to_add= \"د\" \n",
    "            chr_to_change=\"ذ\"\n",
    "        if \"غ\" == i:\n",
    "            chr_to_add= \"ع\"\n",
    "            chr_to_change=\"غ\"\n",
    "        if \"ض\" == i:\n",
    "            chr_to_add= \"ص\"\n",
    "            chr_to_change=\"ض\"\n",
    "        \n",
    "        index_of_old=word.index(chr_to_change)\n",
    "        new_word=''\n",
    "        if chr_to_change !='' and chr_to_add !='':\n",
    "            new_word=word.replace(chr_to_change,chr_to_add)\n",
    "        if new_word not in list_of_distractors and new_word !='':\n",
    "             list_of_distractors.append(new_word)\n",
    "    return list_of_distractors\n",
    "print(dotCountChanger(\"النمور\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إلنمار', 'ألنمار', 'النمإر', 'النمأر']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\" function to change the shape of the letter [\"ا\",\"إ\",\"أ\"] if occured in a  given word  \"\"\"\n",
    "def AlefShapeChanger(word):\n",
    "    listOfDistractors=[]\n",
    "    listAlef = [\"ا\",\"إ\",\"أ\"]\n",
    "#     word=araby.strip_tashkeel(word)\n",
    "    for i in range (len(word)):\n",
    "        if (word[i]) in listAlef:\n",
    "            for k in listAlef:\n",
    "                if (word[:i]+k+word[i+1:]) not in listOfDistractors:\n",
    "                    listOfDistractors.append(word[:i]+k+word[i+1:])\n",
    "    if word in listOfDistractors:\n",
    "        listOfDistractors.remove(word)\n",
    "    return listOfDistractors\n",
    "AlefShapeChanger(\"النمار\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'النماره'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\" function to change the last letter in the word if it is  ه with  ة  and vice versa\"\"\"\n",
    "def lastLetterChanger(word):\n",
    "    list_of_letterswDots=[\"ت\",\"ن\",\"س\",\"ف\",\"ب\",\"ح\",\"خ\",\"ز\",\"ط\",\"د\",\"ع\",\"ث\",\"ش\",\"ق\",\"ي\",\"ج\",\"ر\",\"ظ\",\"ذ\" ,\"غ\" ]\n",
    "    word=araby.strip_tashkeel(word)\n",
    "    list_of_distractors=[]\n",
    "    temp=\"\"\n",
    "    for i in range (2,len(word)):\n",
    "        if \"ه\" == word[-1] :\n",
    "            word=word[:-1]+\"ة\"\n",
    "            break\n",
    "        if \"ة\" == word[-1] :\n",
    "            word=word[:-1]+\"ه\"\n",
    "            break\n",
    "    return word\n",
    "lastLetterChanger(\"النمارة\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6000 sentences data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel(\"7000 sentences Corpus with IDs (1).xlsx\",usecols=['ID','Arabic'])\n",
    "sep = '='\n",
    "temp=[]\n",
    "df=df.dropna()\n",
    "for i in df['Arabic']:\n",
    "    temp.append(i.split(sep, 1)[0])\n",
    "df['Arabic']=temp\n",
    "cleaned_sentences=[]\n",
    "for i in df['Arabic']:\n",
    "    x=i\n",
    "    for k in x:\n",
    "        if k in ['(',')']:\n",
    "            openBracket=x.index('(')\n",
    "            closeBracket = x.index(')')\n",
    "            x=x[:openBracket]+x[(closeBracket+1):]\n",
    "            break\n",
    "    cleaned_sentences.append(x.replace(\"  \", \" \"))\n",
    "df['Arabic_cleaned']=(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises creation and dataset output for verb conjugation exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# the list to be exported as dataframe\n",
    "finalDataFrameList=[]\n",
    "cpt = 0\n",
    "for ind in df.index:\n",
    "    \n",
    "    print(cpt)\n",
    "    # sentence with diacritics \n",
    "    Full_sentence=df['Arabic_cleaned'][ind]########Full_sentence\n",
    "    tokenizedSentence= araby.tokenize(Full_sentence)\n",
    "    # sentence after removing diacritics\n",
    "    sentence_stripped=araby.strip_tashkeel(Full_sentence)\n",
    "    #POS tagging the sentence without diacritics\n",
    "    dictt=nlp(sentence_stripped).to_dict()[0]\n",
    "    # converting the sentence without diacritics to a list\n",
    "    tokenized_sentence_stripped=araby.tokenize(sentence_stripped)\n",
    "    if len(tokenizedSentence)>2:\n",
    "\n",
    "\n",
    "        if tokenizedSentence[-1] in ['.','؟','!'] and tokenized_sentence_stripped[-1] in ['.','؟','!']:\n",
    "\n",
    "            # getting the base of the verb in the sentence\n",
    "            list_of_verbs=[]\n",
    "            list_of_baseVerbs=[]\n",
    "            list_of_verbIndex=[]\n",
    "            for i in dictt:\n",
    "                if 'upos' in i:\n",
    "                    if i['upos']=='VERB':\n",
    "                        if i['text'] in tokenized_sentence_stripped:\n",
    "                            # getting the index of the verb in the list of the sentence without diacritics\n",
    "                            list_of_verbIndex.append(tokenized_sentence_stripped.index(i['text']))\n",
    "                            list_of_verbs.append(i['text'])\n",
    "                            list_of_baseVerbs.append(i['lemma'])\n",
    "\n",
    "\n",
    "            # a loop over the sentence, if the sentence has more than one verb, more than one exercise with be created\n",
    "            for verb in range(len(list_of_verbs)):\n",
    "                if len(araby.tokenize(Full_sentence)) >list_of_verbIndex[verb]:\n",
    "                    # saving the correct answer in Right_answer after getting its index\n",
    "                    Right_answer=araby.tokenize(Full_sentence)[list_of_verbIndex[verb]]########Right_answer\n",
    "                    \n",
    "                    # using the library libqutrub to conjugate a verb\n",
    "                    future_type =u\"\"\n",
    "                    all = False # all tenses\n",
    "                    past = True\n",
    "                    future=True\n",
    "                    passive =False\n",
    "                    imperative=False\n",
    "                    future_moode= False\n",
    "                    confirmed=False\n",
    "                    transitive =False\n",
    "                    \"\"\"\"all = True # conjugate in all arabic tenses.\n",
    "                        past = True # conjugate in past tense ألماضي\n",
    "                        future=True # conjugate in arabic present and future tenses المضارع\n",
    "                        passive =True # conjugate in passive voice المبني للمجهول\n",
    "                        imperative=True # conjugate in imperative tense الأمر\n",
    "                        future_moode= True # conjugate in future moode tenses المضارع المنصوب والمجزوم\n",
    "                        confirmed=True # conjugate in confirmed cases tense المؤكّد\n",
    "                        transitive =True # the verb transitivity \"\"\"\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    display_format=\"DICT\" #output the conjugation table as DICT\n",
    "                    ''' display format forms:\n",
    "                        'Text':\n",
    "                        'HTML':\n",
    "                        'HTMLColoredDiacritics':\n",
    "                        'DICT':\n",
    "                        'CSV':\n",
    "                        'GUI':\n",
    "                        'TABLE':\n",
    "                        'XML':\n",
    "                        'TeX':\n",
    "                        'ROWS': '''\n",
    "                    # calling the function .conjugator.conjugate() to conjugate the verb\n",
    "                    table = libqutrub.conjugator.conjugate(araby.strip_tashkeel(list_of_baseVerbs[verb]),future_type,\n",
    "                    all,past,future,passive,imperative,future_moode,confirmed,transitive,display_format);\n",
    "                        \n",
    "                    if table is None:\n",
    "                        Right_answer=araby.tokenize(Full_sentence)[list_of_verbIndex[verb]]########Right_answer\n",
    "                        future_type =u\"\"\n",
    "                        all = False # all tenses\n",
    "                        past = True\n",
    "                        future=True\n",
    "                        passive =False\n",
    "                        imperative=False\n",
    "                        future_moode= False\n",
    "                        confirmed=False\n",
    "                        transitive =False\n",
    "                        display_format=\"DICT\"\n",
    "                        table = libqutrub.conjugator.conjugate((list_of_baseVerbs[verb]),future_type,\n",
    "                        all,past,future,passive,imperative,future_moode,confirmed,transitive,display_format);\n",
    "\n",
    "\n",
    "                    #loop to create the list of the displayed question with base verb and \"...\"\n",
    "                    tokenizedSentenceTemp=[]\n",
    "                    if tokenizedSentence.index(Right_answer)!=0 and tokenizedSentence.index(Right_answer)!=len(tokenizedSentence)-1:\n",
    "                        for i in tokenizedSentence:\n",
    "                            if araby.strip_tashkeel(i) == araby.strip_tashkeel(Right_answer):\n",
    "                                tokenizedSentenceTemp.append(\"...\"+'('+list_of_baseVerbs[verb]+')')\n",
    "                            else:\n",
    "                                tokenizedSentenceTemp.append(i)\n",
    "\n",
    "\n",
    "\n",
    "                        #list of all the verbs conjugated in present simpla and past simple with all pronouns \n",
    "                        # with removing the ones identical in letters to the correct answer\n",
    "                        if table is not None:\n",
    "                            listOfAllConjugations = list(table['الماضي المعلوم'].values())+list(table['المضارع المعلوم'].values())\n",
    "                            listOfDistractors=[]\n",
    "                            for i in listOfAllConjugations:\n",
    "                                # checking if the distractor to be added isn't identical to the correct answer in letters\n",
    "                                  if araby.strip_tashkeel(i) != araby.strip_tashkeel(Right_answer) and i not in listOfDistractors :\n",
    "                                        listOfDistractors.append(i)\n",
    "\n",
    "\n",
    "                            # picking 3 distractors at random from the list of all the distractors\n",
    "                            listOfChoices = random.sample(listOfDistractors, 3)\n",
    "                            dist_1=listOfChoices[0]########dist_1\n",
    "                            dist_2=listOfChoices[1]########dist_2\n",
    "                            dist_3=listOfChoices[2]########dist_3\n",
    "                            listOfChoices.append(Right_answer)\n",
    "                            #shuffling the list of the 4 choices\n",
    "                            listOfChoices = random.sample(listOfChoices,4)\n",
    "\n",
    "                            cpt+=1\n",
    "\n",
    "                            Sentence_with_blank=(' '.join(map(str,tokenizedSentenceTemp)))########Sentence_with_blank\n",
    "\n",
    "                            Exo_type_id = 0\n",
    "\n",
    "                            Exo_type = \"Cloze_Test_+_MCQ\"\n",
    "\n",
    "                            Exo_objective= \"Grammar\"\n",
    "\n",
    "                            Exo_focus = \"Verb_conjugation\"\n",
    "\n",
    "                            Exo_id = cpt\n",
    "\n",
    "                            Source_format= 'Text'\n",
    "\n",
    "                            Target_format= 'Text'\n",
    "\n",
    "                            Source_sentence_id = df['ID'][ind]\n",
    "\n",
    "                            Source_word_id = 0\n",
    "\n",
    "                            Source_lang= 'French'\n",
    "\n",
    "                            Target_lang= 'Arabic'\n",
    "\n",
    "                            Instruction = \"Conjugate the ver between brackets in the correct tense and with the suitable pronoun\"\n",
    "\n",
    "                            Explanation = \"\"\n",
    "\n",
    "                            Difficulty = 0\n",
    "\n",
    "                            Remediation = \"\"\n",
    "\n",
    "                            propositions = '-'.join(listOfChoices)\n",
    "\n",
    "\n",
    "\n",
    "                            finalList=[Exo_type_id, Exo_type, Exo_objective,Exo_focus,Exo_id,Source_format,Target_format,\n",
    "                                       Source_sentence_id ,Source_word_id, Source_lang,Target_lang,Full_sentence,Instruction,Sentence_with_blank,\n",
    "                                       Right_answer,dist_1,dist_2,dist_3, propositions,Explanation, Difficulty, Remediation]\n",
    "\n",
    "                            finalDataFrameList.append(finalList)\n",
    "finalDataFrame=pd.DataFrame(finalDataFrameList,columns=[\"Exo_type_id\",\"Exo_type\",'Exo_objective',\"Exo_focus\",'Exo_id','Source_format','Target_format',\n",
    "                                                             \"Source_sentence_id\" ,\"Source_word_id\", 'Source_lang','Target_lang','Full_sentence',\"Instruction\", 'Sentence_with_blank',\n",
    "                                                             'Right_answer', 'dist_1', 'dist_2', 'dist_3', \"Propositions\",\"Explanation\", \"Difficulty\", \"Remediation\"])\n",
    "finalDataFrame.to_excel('finalDataFrameVerbConjExos6k.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb conjugation creation with focus point on gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "## for the dataset\n",
    "finalDataFrameList=[]\n",
    "cpt = 0\n",
    "for ind in df.index:\n",
    "    # sentence with diacritics \n",
    "    Full_sentence=df['Arabic_cleaned'][ind]########Full_sentence\n",
    "    tokenizedSentence= araby.tokenize(Full_sentence)\n",
    "    # sentence after removing diacritics\n",
    "    sentence_stripped=araby.strip_tashkeel(Full_sentence)\n",
    "    #POS tagging the sentence without diacritics\n",
    "    dictt=nlp(sentence_stripped).to_dict()[0]\n",
    "    # converting the sentence without diacritics to a list\n",
    "    tokenized_sentence_stripped=araby.tokenize(sentence_stripped)\n",
    "    \n",
    "    \n",
    "    # getting the base of the verb in the sentence\n",
    "    list_of_verbs=[]\n",
    "    list_of_baseVerbs=[]\n",
    "    list_of_verbIndex=[]\n",
    "    list_of_feats=[]\n",
    "    for i in dictt:\n",
    "        if 'upos' in i:\n",
    "            if i['upos']=='VERB':\n",
    "                if i['text'] in tokenized_sentence_stripped:\n",
    "                    # getting the index of the verb in the list of the sentence without diacritics\n",
    "                    list_of_verbIndex.append(tokenized_sentence_stripped.index(i['text']))\n",
    "                    list_of_verbs.append(i['text'])\n",
    "                    list_of_baseVerbs.append(i['lemma'])  \n",
    "                 \n",
    "                    \n",
    "    for verb in range(len(list_of_verbs)):\n",
    "    \n",
    "\n",
    "        if len(araby.tokenize(Full_sentence)) >list_of_verbIndex[verb]:\n",
    "            if tokenizedSentence[-1] in ['.','؟','!'] and tokenized_sentence_stripped[-1] in ['.','؟','!']:\n",
    "\n",
    "                # saving the correct answer in Right_answer after getting its index\n",
    "                Right_answer=araby.tokenize(Full_sentence)[list_of_verbIndex[verb]]########Right_answer\n",
    "                future_type =u\"\"\n",
    "                all = False # all tenses\n",
    "                past = True\n",
    "                future=True\n",
    "                passive =False\n",
    "                imperative=False\n",
    "                future_moode= False\n",
    "                confirmed=False\n",
    "                transitive =False\n",
    "                display_format=\"DICT\"\n",
    "                table = libqutrub.conjugator.conjugate(araby.strip_tashkeel(list_of_baseVerbs[verb]),future_type,\n",
    "                all,past,future,passive,imperative,future_moode,confirmed,transitive,display_format);\n",
    "\n",
    "                #loop to create the list of the displayed question with base verb and \"...\"\n",
    "                tokenizedSentenceTemp=[]\n",
    "                if tokenizedSentence.index(Right_answer)!=0 and tokenizedSentence.index(Right_answer)!=len(tokenizedSentence)-1:\n",
    "                    for i in tokenizedSentence:\n",
    "                        if araby.strip_tashkeel(i) == araby.strip_tashkeel(Right_answer):\n",
    "                            tokenizedSentenceTemp.append(\"...\"+'('+list_of_baseVerbs[verb]+')')\n",
    "                        else:\n",
    "                            tokenizedSentenceTemp.append(i)\n",
    "\n",
    "                    #list of all the verbs conjugated in present simpla and past simple with all pronouns \n",
    "                    # with removing the ones identical in letters to the correct answer\n",
    "                    if table is not None:\n",
    "                        listOfMasc=[(table['الماضي المعلوم']['هو']),(table['المضارع المعلوم']['هم'])]\n",
    "\n",
    "                        listOfFem=[(table['المضارع المعلوم']['هي']),(table['المضارع المعلوم']['هن'])]\n",
    "\n",
    "                        listOfDistractors=[]\n",
    "                        # checking if the distractor to be added isn't identical to the correct answer in letters\n",
    "                        for i in listOfMasc+listOfFem:\n",
    "\n",
    "                            if araby.strip_tashkeel(Right_answer) != araby.strip_tashkeel(i):\n",
    "                                listOfDistractors.append(i)\n",
    "\n",
    "                        # picking 3 distractors at random from the list of all the distractors\n",
    "                        if len(listOfDistractors)>2:\n",
    "                            listOfChoices = random.sample(listOfDistractors, 3)\n",
    "                            dist_1=listOfChoices[0]########dist_1\n",
    "                            dist_2=listOfChoices[1]########dist_2\n",
    "                            dist_3=listOfChoices[2]########dist_3\n",
    "                            listOfChoices.append(Right_answer)\n",
    "                            #shuffling the list of the 4 choices\n",
    "                            listOfChoices = random.sample(listOfChoices,4)\n",
    "\n",
    "                            cpt+=1\n",
    "\n",
    "                            Sentence_with_blank=(' '.join(map(str,tokenizedSentenceTemp)))########Sentence_with_blank\n",
    "\n",
    "                            Exo_type_id = 35\n",
    "\n",
    "                            Exo_type = \"Cloze_Test_+_MCQ\"\n",
    "\n",
    "                            Exo_objective= \"Verb_Conjugation\"\n",
    "\n",
    "                            Exo_focus = \"Gender\"\n",
    "\n",
    "                            Exo_id = cpt\n",
    "\n",
    "                            Source_format= 'Text'\n",
    "\n",
    "                            Target_format= 'Text'\n",
    "\n",
    "                            Source_sentence_id = df['ID'][ind]\n",
    "\n",
    "                            Source_word_id = 0\n",
    "\n",
    "                            Source_lang= 'French'\n",
    "\n",
    "                            Target_lang= 'Arabic'\n",
    "\n",
    "                            Instruction = \"Conjugate the verb between brackets:\"\n",
    "\n",
    "                            Explanation = \"\"\n",
    "\n",
    "                            Difficulty = 0\n",
    "\n",
    "                            Remediation = \"\"\n",
    "\n",
    "                            propositions = '-'.join(listOfChoices)\n",
    "\n",
    "\n",
    "                            finalList=[Exo_type_id, Exo_type, Exo_objective,Exo_focus,Exo_id,Source_format,Target_format,\n",
    "                                       Source_sentence_id ,Source_word_id, Source_lang,Target_lang,Full_sentence,Instruction,Sentence_with_blank,\n",
    "                                       Right_answer,dist_1,dist_2,dist_3,propositions, Explanation, Difficulty, Remediation]\n",
    "\n",
    "                            finalDataFrameList.append(finalList)\n",
    "\n",
    "                            print(cpt)\n",
    "finalDataFrame=pd.DataFrame(finalDataFrameList,columns=[\"Exo_type_id\",\"Exo_type\",'Exo_objective',\"Exo_focus\",'Exo_id','Source_format','Target_format',\n",
    "                                                             \"Source_sentence_id\" ,\"Source_word_id\", 'Source_lang','Target_lang','Full_sentence',\"Instruction\", 'Sentence_with_blank',\n",
    "                                                             'Right_answer', 'dist_1', 'dist_2', 'dist_3', \"Propositions\",\"Explanation\", \"Difficulty\", \"Remediation\"])\n",
    "finalDataFrame.to_excel('finalDataFrameVerbConjExosGenderFocus.xlsx')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercises creation and dataset output for nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "finalDataFrameList=[]\n",
    "cpt = 0\n",
    "for ind in df.index:\n",
    "    # sentence with diacritics \n",
    "    Full_sentence=df['Arabic_cleaned'][ind]########Full_sentence\n",
    "    tokenizedSentence= araby.tokenize(Full_sentence)\n",
    "\n",
    "    # sentence after removing diacritics\n",
    "    sentence_stripped=araby.strip_tashkeel(Full_sentence)\n",
    "\n",
    "    #POS tagging the sentence without diacritics\n",
    "    dictt=nlp(sentence_stripped).to_dict()[0]\n",
    "\n",
    "    # converting the sentence without diacritics to a list\n",
    "    tokenized_sentence_stripped=araby.tokenize(sentence_stripped)\n",
    "    \n",
    "    \n",
    "\n",
    "    if len(tokenizedSentence)>2:\n",
    "        \n",
    "        if tokenizedSentence[-1]  in ['.','؟','!'] and tokenized_sentence_stripped[-1]  in ['.','؟','!']:\n",
    "            list_of_nounIndex=[]\n",
    "            list_of_nouns=[]\n",
    "            # getting the base of the verb in the sentence\n",
    "            for i in dictt:\n",
    "                if 'upos' in i:\n",
    "                    if i['upos']=='NOUN' and len(i['text'])>2:\n",
    "                        if i['text'] in tokenized_sentence_stripped:\n",
    "                            list_of_nounIndex.append(tokenized_sentence_stripped.index(i['text']))\n",
    "                            list_of_nouns.append(i['text'])\n",
    "                            \n",
    "            for noun in range (len(list_of_nounIndex)):                \n",
    "                # saving the correct answer in Right_answer after getting its index\n",
    "                if list_of_nounIndex[noun] !=0 and list_of_nounIndex[noun] != len(tokenizedSentence)-2:\n",
    "                    Right_answer=araby.tokenize(Full_sentence)[list_of_nounIndex[noun] ]########Right_answer\n",
    "\n",
    "                    if Right_answer not in ['.','',' '] and len(Right_answer)>3 :\n",
    "\n",
    "\n",
    "                        #loop to create the list of the displayed question with base verb and \"...\"\n",
    "                        tokenizedSentenceTemp=[]\n",
    "                        for i in tokenizedSentence:\n",
    "                            if araby.strip_tashkeel(i) == araby.strip_tashkeel(Right_answer):\n",
    "                                tokenizedSentenceTemp.append(\"...\")\n",
    "                            else:\n",
    "                                tokenizedSentenceTemp.append(i)\n",
    "\n",
    "\n",
    "\n",
    "                        #list of all the verbs conjugated in present simpla and past simple with all pronouns \n",
    "                        # with removing the ones identical in letters to the correct answer\n",
    "                        listOfAllDistractors=[]\n",
    "                        listOfAllDistractors+=dotCountChanger(Right_answer)\n",
    "                        if len(listOfAllDistractors)<4:\n",
    "                            listOfAllDistractors.append(lastLetterChanger(Right_answer))\n",
    "                            if len(listOfAllDistractors)<4:\n",
    "                                listOfAllDistractors+=AlefShapeChanger(Right_answer)\n",
    "\n",
    "\n",
    "                        if len(listOfAllDistractors)>3:\n",
    "\n",
    "                            listOfDistractors=[]\n",
    "                            for i in listOfAllDistractors:\n",
    "                                # checking if the distractor to be added isn't identical to the correct answer in letters\n",
    "                                  if araby.strip_tashkeel(i) != araby.strip_tashkeel(Right_answer):\n",
    "                                      listOfDistractors.append(i)\n",
    "                            if len(listOfDistractors)>2:\n",
    "                                # picking 3 distractors at random from the list of all the distractors\n",
    "                                listOfChoices = random.sample(listOfDistractors, 3)\n",
    "                                dist_1=listOfChoices[0]########dist_1\n",
    "                                dist_2=listOfChoices[1]########dist_2\n",
    "                                dist_3=listOfChoices[2]########dist_3\n",
    "                                listOfChoices.append(Right_answer)\n",
    "                                listOfChoices = random.sample(listOfChoices, 4)\n",
    "                                Sentence_with_blank=(' '.join(map(str,tokenizedSentenceTemp)))########Sentence_with_blank\n",
    "\n",
    "                                Exo_type_id = 0\n",
    "\n",
    "                                Exo_type = \"Cloze_Test + MCQ\"\n",
    "\n",
    "                                Exo_objective= \"Vocabulary\"\n",
    "\n",
    "                                Exo_focus = \"Noun_spelling\"\n",
    "\n",
    "                                Exo_id = cpt\n",
    "\n",
    "                                Source_format= 'Text'\n",
    "\n",
    "                                Target_format= 'Text'\n",
    "\n",
    "                                Source_sentence_id = df['ID'][ind]\n",
    "\n",
    "                                Source_word_id = 0\n",
    "\n",
    "                                Source_lang= 'French'\n",
    "\n",
    "                                Target_lang= 'Arabic'\n",
    "\n",
    "                                Instruction = \"Choose the correct answer:\"\n",
    "\n",
    "                                Explanation = \"\"\n",
    "\n",
    "                                Difficulty = 0\n",
    "\n",
    "                                Remediation = \"\"\n",
    "                                \n",
    "                                propositions = '-'.join(listOfChoices)\n",
    "\n",
    "\n",
    "                                finalList=[Exo_type_id, Exo_type, Exo_objective,Exo_focus,Exo_id,Source_format,Target_format,\n",
    "                                           Source_sentence_id ,Source_word_id, Source_lang,Target_lang,Full_sentence,Instruction,Sentence_with_blank,\n",
    "                                           Right_answer,dist_1,dist_2,dist_3, propositions,Explanation, Difficulty, Remediation]\n",
    "                                finalDataFrameList.append(finalList)\n",
    "                                cpt+=1\n",
    "                                print(cpt)\n",
    "finalDataFrame=pd.DataFrame(finalDataFrameList,columns=[\"Exo_type_id\",\"Exo_type\",'Exo_objective',\"Exo_focus\",'Exo_id','Source_format','Target_format',\n",
    "                                                 \"Source_sentence_id\" ,\"Source_word_id\", 'Source_lang','Target_lang','Full_sentence',\"Instruction\", 'Sentence_with_blank',\n",
    "                                                 'Right_answer', 'dist_1', 'dist_2', 'dist_3', \"Propositions\",\"Explanation\", \"Difficulty\", \"Remediation\"])\n",
    "\n",
    "finalDataFrame.to_excel('finalDataFrameNounExos.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercises creation and dataset output for prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "finalDataFrameList=[]\n",
    "listOfPrepositions=[]\n",
    "features=''\n",
    "cpt = 0\n",
    "for ind in df.index:\n",
    "    cpt+=1\n",
    "    # sentence with diacritics \n",
    "    Full_sentence=df['Arabic_cleaned'][ind]########Full_sentence\n",
    "    tokenizedSentence= araby.tokenize(Full_sentence)\n",
    "    # sentence after removing diacritics\n",
    "    sentence_stripped=araby.strip_tashkeel(Full_sentence)\n",
    "    #POS tagging the sentence without diacritics\n",
    "    dictt=nlp(sentence_stripped).to_dict()[0]\n",
    "    # converting the sentence without diacritics to a list\n",
    "    tokenized_sentence_stripped=araby.tokenize(sentence_stripped)\n",
    "    print(cpt)\n",
    "    \n",
    "    # getting the base of the verb in the sentence\n",
    "    for i in dictt:\n",
    "        if 'upos' in i:\n",
    "            if i['upos']=='ADP':\n",
    "                if i['text'] in tokenized_sentence_stripped:\n",
    "                    mainPrepositionIndex=tokenized_sentence_stripped.index(i['text']) \n",
    "                    features=str(i['feats']).split('|')\n",
    "                    preposition=i['lemma']\n",
    "    for i in features:\n",
    "        if i[:7] == 'AdpType':\n",
    "            if (i.split('=')[1]) == \"Prep\":\n",
    "                listOfPrepositions.append(preposition)\n",
    "listOfPrepositionsUniquevals= []\n",
    "for i in listOfPrepositions:\n",
    "    if i not in listOfPrepositionsUniquevals:\n",
    "        listOfPrepositionsUniquevals.append(i)\n",
    "listOfPreps=listOfPrepositionsUniquevals         \n",
    "\n",
    "\n",
    "# listOfPreps=['فِي','إِزَاءَ','بَينَ', 'مِن','إِلَى', 'لَدَى','أَثنَاءَ','دُونَ','رَغمَ','عَلَى','لِ','خِلَالَ','عَن','دَاخِلَ','حِينَ','قَبلَ','بَعدَ','خَلفَ'\n",
    "#              ,'فَوقَ','مُنذُ','خَارِجَ','عِندَ','نَحوَ','مِثلَ','مَعَ','طِوَالَ','قُربَ','طِيلَةَ','تَحتَ','ضِمنَ','أَمَامَ','سِوَى','لَعل','حَولَ'\n",
    "#              ,'جَرَّاء','لِكَّ','وِفقَ','ضِدَّ','تُجَاهَ','حَوَالَى','حَسَبَ','فَئَى','إِثرَ','عَبرَ','تَلوَ','قُبَالَةَ','وَرَاءَ','جَنب']\n",
    "## for the dataset\n",
    "finalDataFrameList=[]\n",
    "cpt = 0\n",
    "for ind in df.index:\n",
    "    # sentence with diacritics \n",
    "    Full_sentence=df['Arabic_cleaned'][ind]########Full_sentence\n",
    "    tokenizedSentence= araby.tokenize(Full_sentence)\n",
    "    # sentence after removing diacritics\n",
    "    sentence_stripped=araby.strip_tashkeel(Full_sentence)\n",
    "    #POS tagging the sentence without diacritics\n",
    "    dictt=nlp(sentence_stripped).to_dict()[0]\n",
    "    # converting the sentence without diacritics to a list\n",
    "    tokenized_sentence_stripped=araby.tokenize(sentence_stripped)\n",
    "    if tokenizedSentence[-1]  in ['.','؟','!'] and tokenized_sentence_stripped[-1]  in ['.','؟','!']:\n",
    "#         preposition=''\n",
    "#         Right_answer = \"\"\n",
    "#         # getting the base of the verb in the sentence\n",
    "#         for i in dictt:\n",
    "#             if 'upos' in i:\n",
    "#                 if i['upos']=='ADP':\n",
    "#                     if i['text'] in tokenized_sentence_stripped:\n",
    "#                         mainPrepositionIndex=tokenized_sentence_stripped.index(i['text']) \n",
    "#                         features=str(i['feats']).split('|')\n",
    "#                         Right_answer = i['lemma']\n",
    "                        \n",
    "                        \n",
    "        list_of_rightAnswers=[]\n",
    "        list_of_prepIndex=[]\n",
    "        list_of_feats=[]\n",
    "        for i in dictt:\n",
    "            if 'upos' in i:\n",
    "                if i['upos']=='ADP':\n",
    "                    if i['text'] in tokenized_sentence_stripped:\n",
    "                        list_of_prepIndex.append(tokenized_sentence_stripped.index(i['text'])) \n",
    "                        list_of_rightAnswers.append(i['lemma'])\n",
    "                        list_of_feats.append(str(i['feats']).split('|'))\n",
    "        \n",
    "        for prep in range(len(list_of_prepIndex)):\n",
    "            if list_of_prepIndex[prep]!=0 and list_of_prepIndex[prep]!= len(tokenized_sentence_stripped)-1:\n",
    "\n",
    "                if list_of_rightAnswers[prep] != \"\":\n",
    "                    for i in list_of_feats[prep]:\n",
    "                        if i[:7] == 'AdpType':\n",
    "                            if (i.split('=')[1]) == \"Prep\":\n",
    "\n",
    "            #                     #loop to create the list of the displayed question with base verb and \"...\"\n",
    "                                tokenizedSentenceTemp=[]\n",
    "                                for i in tokenizedSentence:\n",
    "                                    if araby.strip_tashkeel(i) == araby.strip_tashkeel(list_of_rightAnswers[prep]) and (\"...\") not in tokenizedSentenceTemp:\n",
    "                                        tokenizedSentenceTemp.append(\"...\")\n",
    "\n",
    "                                    else:\n",
    "                                        tokenizedSentenceTemp.append(i)\n",
    "\n",
    "                                listOfDistractors=[]\n",
    "                                # checking if the distractor to be added isn't identical to the correct answer in letters\n",
    "                                for i in listOfPreps:\n",
    "                                    if araby.strip_tashkeel(list_of_rightAnswers[prep]) != araby.strip_tashkeel(i):\n",
    "                                        listOfDistractors.append(i)\n",
    "\n",
    "                                # picking 3 distractors at random from the list of all the distractors\n",
    "                                if len(listOfDistractors)>2:\n",
    "                                    listOfChoices = random.sample(listOfDistractors, 3)\n",
    "                                    dist_1=listOfChoices[0]########dist_1\n",
    "                                    dist_2=listOfChoices[1]########dist_2\n",
    "                                    dist_3=listOfChoices[2]########dist_3\n",
    "                                    listOfChoices.append(list_of_rightAnswers[prep])\n",
    "                                    #shuffling the list of the 4 choices\n",
    "                                    listOfChoices = random.sample(listOfChoices,4)\n",
    "\n",
    "                                    cpt+=1\n",
    "\n",
    "                                    Sentence_with_blank=(' '.join(map(str,tokenizedSentenceTemp)))########Sentence_with_blank\n",
    "\n",
    "                                    Exo_type_id = 0\n",
    "\n",
    "                                    Exo_type = \"Cloze_Test + MCQ\"\n",
    "\n",
    "                                    Exo_objective= \"Grammar\"\n",
    "\n",
    "                                    Exo_focus = \"Prepositions\"\n",
    "\n",
    "                                    Exo_id = cpt\n",
    "\n",
    "                                    Source_format= 'Text'\n",
    "\n",
    "                                    Target_format= 'Text'\n",
    "\n",
    "                                    Source_sentence_id = df['ID'][ind]\n",
    "\n",
    "                                    Source_word_id = 0\n",
    "\n",
    "                                    Source_lang= 'French'\n",
    "\n",
    "                                    Target_lang= 'Arabic'\n",
    "\n",
    "                                    Instruction = \"\"\n",
    "\n",
    "                                    Explanation = \"\"\n",
    "\n",
    "                                    Difficulty = 0\n",
    "\n",
    "                                    Remediation = \"\"\n",
    "                                    \n",
    "                                    propositions = '-'.join(listOfChoices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    finalList=[Exo_type_id, Exo_type, Exo_objective,Exo_focus,Exo_id,Source_format,Target_format,\n",
    "                                               Source_sentence_id ,Source_word_id, Source_lang,Target_lang,Full_sentence,Instruction,Sentence_with_blank,\n",
    "                                               list_of_rightAnswers[prep],dist_1,dist_2,dist_3,propositions ,Explanation, Difficulty, Remediation]\n",
    "\n",
    "                                    finalDataFrameList.append(finalList)\n",
    "\n",
    "                                    print(cpt)\n",
    "finalDataFrame=pd.DataFrame(finalDataFrameList,columns=[\"Exo_type_id\",\"Exo_type\",'Exo_objective',\"Exo_focus\",'Exo_id','Source_format','Target_format',\n",
    "                                                             \"Source_sentence_id\" ,\"Source_word_id\", 'Source_lang','Target_lang','Full_sentence',\"Instruction\", 'Sentence_with_blank',\n",
    "                                                             'Right_answer', 'dist_1', 'dist_2', 'dist_3', 'Propositions',\"Explanation\", \"Difficulty\", \"Remediation\"])\n",
    "finalDataFrame.to_excel('finalDataFramePrepositions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercises creation and dataset output for punctutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finalDataFrameList=[]\n",
    "cpt = 0\n",
    "for ind in df.index:\n",
    "    # sentence with diacritics \n",
    "    Full_sentence=df['Arabic_cleaned'][ind]########Full_sentence\n",
    "    tokenizedSentence= araby.tokenize(Full_sentence)\n",
    "    # sentence after removing diacritics\n",
    "    sentence_stripped=araby.strip_tashkeel(Full_sentence)\n",
    "    #POS tagging the sentence without diacritics\n",
    "    dictt=nlp(sentence_stripped).to_dict()[0]\n",
    "    # converting the sentence without diacritics to a list\n",
    "    tokenized_sentence_stripped=araby.tokenize(sentence_stripped)\n",
    "#     ['.','؟','!','،','؛']\n",
    "    punctuationMark=''\n",
    "    mainPunctIndex = 0\n",
    "    # getting the base of the verb in the sentence\n",
    "    for i in dictt:\n",
    "        if 'upos' in i:\n",
    "            if i['upos']=='PUNCT':\n",
    "                if i['text'] in tokenized_sentence_stripped:\n",
    "                    mainPunctIndex=tokenized_sentence_stripped.index(i['text']) \n",
    "                    punctuationMark = i['text']\n",
    "                    break\n",
    "                break\n",
    "            \n",
    "    if tokenizedSentence[0] not in ['.','؟','!','،','؛','\"']:\n",
    "        if punctuationMark !='':\n",
    "    \n",
    "            #loop to create the list of the displayed question with  \"...\"\n",
    "            tokenizedSentenceTemp=[]\n",
    "            for i in tokenizedSentence:\n",
    "                if i == punctuationMark and (\"...\") not in tokenizedSentenceTemp:\n",
    "                    tokenizedSentenceTemp.append(\"...\")\n",
    "\n",
    "                else:\n",
    "                    tokenizedSentenceTemp.append(i)\n",
    "\n",
    "            listOfDistractors=[]\n",
    "            # checking if the distractor to be added isn't identical to the correct answer in letters\n",
    "            if punctuationMark in ['.','؟','!']:\n",
    "                for i in ['.','؟','!','،'] :\n",
    "                    if punctuationMark != i:\n",
    "                        listOfDistractors.append(i)\n",
    "            else: \n",
    "                for i in ['.','،','؛','؟']:\n",
    "                    if punctuationMark != i:\n",
    "                        listOfDistractors.append(i)\n",
    "\n",
    "            # picking 3 distractors at random from the list of all the distractors\n",
    "            if len(listOfDistractors)>2:\n",
    "                listOfChoices = random.sample(listOfDistractors, 3)\n",
    "                dist_1=listOfChoices[0]########dist_1\n",
    "                dist_2=listOfChoices[1]########dist_2\n",
    "                dist_3=listOfChoices[2]########dist_3\n",
    "                listOfChoices.append(punctuationMark)\n",
    "                #shuffling the list of the 4 choices\n",
    "                listOfChoices = random.sample(listOfChoices,4)\n",
    "\n",
    "                cpt+=1\n",
    "\n",
    "                Sentence_with_blank=(' '.join(map(str,tokenizedSentenceTemp)))########Sentence_with_blank\n",
    "\n",
    "                Exo_type_id = 0\n",
    "\n",
    "                Exo_type = \"Cloze_Test + MCQ\"\n",
    "\n",
    "                Exo_objective= \"Grammar\"\n",
    "\n",
    "                Exo_focus = \"Punctuation\"\n",
    "\n",
    "                Exo_id = cpt\n",
    "\n",
    "                Source_format= 'text'\n",
    "\n",
    "                Target_format= 'text'\n",
    "\n",
    "                Source_sentence_id = df['ID'][ind]\n",
    "\n",
    "                Source_word_id = 0\n",
    "\n",
    "                Source_lang= 'French'\n",
    "\n",
    "                Target_lang= 'Arabic'\n",
    "\n",
    "                Instruction = \"Choose the correct punctuation mark\"\n",
    "\n",
    "                Explanation = \"\"\n",
    "\n",
    "                Difficulty = 0\n",
    "\n",
    "                Remediation = \"\"\n",
    "    \n",
    "                propositions = '-'.join(listOfChoices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                finalList=[Exo_type_id, Exo_type, Exo_objective,Exo_focus,Exo_id,Source_format,Target_format,\n",
    "                           Source_sentence_id ,Source_word_id, Source_lang,Target_lang,Full_sentence,Instruction,Sentence_with_blank,\n",
    "                           punctuationMark,dist_1,dist_2,dist_3, propositions ,Explanation, Difficulty, Remediation]\n",
    "\n",
    "                finalDataFrameList.append(finalList)\n",
    "\n",
    "                print(cpt)\n",
    "finalDataFrame=pd.DataFrame(finalDataFrameList,columns=[\"Exo_type_id\",\"Exo_type\",'Exo_objective',\"Exo_focus\",'Exo_id','Source_format','Target_format',\n",
    "                                                             \"Source_sentence_id\" ,\"Source_word_id\", 'Source_lang','Target_lang','Full_sentence',\"Instruction\", 'Sentence_with_blank',\n",
    "                                                             'Right_answer', 'dist_1', 'dist_2', 'dist_3', \"Propositions\" , \"Explanation\", \"Difficulty\", \"Remediation\"])\n",
    "finalDataFrame.to_excel('finalDataFramePunctuation.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
